{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 pictures converted.\n"
     ]
    }
   ],
   "source": [
    "# create list\n",
    "healthy_arrays = []\n",
    "# define filepath for Dog class\n",
    "healthy_path = 'Data/Healthy/'\n",
    "\n",
    "# convert each image to normalized array and store\n",
    "count = 0\n",
    "for file in os.listdir(healthy_path):\n",
    "    if count < 500:\n",
    "        try:\n",
    "            # target_size automatically resizes each img on import\n",
    "            healthy = load_img(healthy_path + file, target_size=(256, 256))\n",
    "            healthy_arr = img_to_array(healthy) / 255\n",
    "            healthy_arrays.append(healthy_arr)\n",
    "        except:\n",
    "            print(f'Error for file: {file}')\n",
    "        count +=1\n",
    "\n",
    "print(f'{len(healthy_arrays)} pictures converted.')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 pictures converted.\n"
     ]
    }
   ],
   "source": [
    "# create list\n",
    "diseased_arrays = []\n",
    "# define filepath for Dog class\n",
    "diseased_path = 'Data/Diseased/'\n",
    "\n",
    "# convert each image to normalized array and store\n",
    "count = 0\n",
    "for file in os.listdir(diseased_path):\n",
    "    if count < 500:\n",
    "        try:\n",
    "            # target_size automatically resizes each img on import\n",
    "            diseased = load_img(diseased_path + file, target_size=(256, 256))\n",
    "            diseased_arr = img_to_array(diseased) / 255\n",
    "            diseased_arrays.append(diseased_arr)\n",
    "        except:\n",
    "            print(f'Error for file: {file}')\n",
    "        count +=1\n",
    "\n",
    "print(f'{len(diseased_arrays)} pictures converted.')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 256, 256, 3)\n",
      "y shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# X should contain both not dogs and dogs\n",
    "X = healthy_arrays + diseased_arrays\n",
    "# convert to array and check shape\n",
    "X_arr = np.array(X)\n",
    "print(f'X shape: {X_arr.shape}')\n",
    "\n",
    "# 1 for dog, 0 for not dog\n",
    "y = [1] * 500 + [0] * 500\n",
    "# convert to array and check shape\n",
    "y = np.array(y)\n",
    "print(f'y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 750 samples, validate on 250 samples\n",
      "Epoch 1/5\n",
      "750/750 [==============================] - 254s 339ms/sample - loss: 2.7835 - accuracy: 0.5240 - val_loss: 0.6955 - val_accuracy: 0.5080\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 286s 382ms/sample - loss: 0.6489 - accuracy: 0.6320 - val_loss: 0.6945 - val_accuracy: 0.5760\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 284s 378ms/sample - loss: 0.5788 - accuracy: 0.6827 - val_loss: 0.5689 - val_accuracy: 0.7240\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 277s 370ms/sample - loss: 0.4266 - accuracy: 0.7880 - val_loss: 0.5050 - val_accuracy: 0.7640\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 312s 416ms/sample - loss: 0.3882 - accuracy: 0.8467 - val_loss: 0.5066 - val_accuracy: 0.7840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc14ff5a950>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu', input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D(pool_size= (2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "# flatten and make dense\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          batch_size=64,\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 254, 254, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 125, 125, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 246016)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                15745088  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 15,783,873\n",
      "Trainable params: 15,783,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nnet)",
   "language": "python",
   "name": "nnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
