{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to process crop, houseplant, and Reddit images for CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 pictures converted.\n"
     ]
    }
   ],
   "source": [
    "# create list for healthy crop images\n",
    "healthycrop_arrays = []\n",
    "# define filepath for healthy crop images\n",
    "healthycrop_path = 'Data/crop_images/healthy/'\n",
    "\n",
    "# convert each image to normalized array and store\n",
    "count = 0\n",
    "for file in os.listdir(healthycrop_path):\n",
    "    if count < 500:\n",
    "        try:\n",
    "            # target_size automatically resizes each img on import\n",
    "            healthy = load_img(healthycrop_path + file, target_size=(256, 256))\n",
    "            healthy_arr = img_to_array(healthy) / 255\n",
    "            healthycrop_arrays.append(healthy_arr)\n",
    "        except:\n",
    "            print(f'Error for file: {file}')\n",
    "        count +=1\n",
    "\n",
    "print(f'{len(healthycrop_arrays)} pictures converted.')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451 pictures converted.\n"
     ]
    }
   ],
   "source": [
    "# create list for healthy houseplant images\n",
    "healthyhouse_arrays = []\n",
    "# define filepath for healthy houseplant images\n",
    "healthyhouse_path = 'Data/houseplant_images/healthy/'\n",
    "\n",
    "# convert each image to normalized array and store\n",
    "count = 0\n",
    "for file in os.listdir(healthyhouse_path):\n",
    "    if count < 451:\n",
    "        try:\n",
    "            # target_size automatically resizes each img on import\n",
    "            healthy = load_img(healthyhouse_path + file, target_size=(256, 256))\n",
    "            healthy_arr = img_to_array(healthy) / 255\n",
    "            healthyhouse_arrays.append(healthy_arr)\n",
    "        except:\n",
    "            print(f'Error for file: {file}')\n",
    "        count +=1\n",
    "\n",
    "print(f'{len(healthyhouse_arrays)} pictures converted.')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 pictures converted.\n"
     ]
    }
   ],
   "source": [
    "# create list for diseased crop images\n",
    "diseasedcrop_arrays = []\n",
    "# define filepath for diseased crop images\n",
    "diseased_path = 'Data/crop_images/diseased/'\n",
    "\n",
    "# convert each image to normalized array and store\n",
    "count = 0\n",
    "for file in os.listdir(diseased_path):\n",
    "    if count < 500:\n",
    "        try:\n",
    "            # target_size automatically resizes each img on import\n",
    "            diseased = load_img(diseased_path + file, target_size=(256, 256))\n",
    "            diseased_arr = img_to_array(diseased) / 255\n",
    "            diseasedcrop_arrays.append(diseased_arr)\n",
    "        except:\n",
    "            print(f'Error for file: {file}')\n",
    "        count +=1\n",
    "\n",
    "print(f'{len(diseasedcrop_arrays)} pictures converted.')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451 pictures converted.\n"
     ]
    }
   ],
   "source": [
    "# create list for wilted houseplant images\n",
    "wiltedhouse_arrays = []\n",
    "# define filepath for wilted houseplant images\n",
    "wilted_path = 'Data/houseplant_images/wilted/'\n",
    "\n",
    "# convert each image to normalized array and store\n",
    "count = 0\n",
    "for file in os.listdir(wilted_path):\n",
    "    if count < 451:\n",
    "        try:\n",
    "            # target_size automatically resizes each img on import\n",
    "            diseased = load_img(wilted_path + file, target_size=(256, 256))\n",
    "            diseased_arr = img_to_array(diseased) / 255\n",
    "            wiltedhouse_arrays.append(diseased_arr)\n",
    "        except:\n",
    "            print(f'Error for file: {file}')\n",
    "        count +=1\n",
    "\n",
    "print(f'{len(wiltedhouse_arrays)} pictures converted.')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for file: .DS_Store\n",
      "140 pictures converted.\n"
     ]
    }
   ],
   "source": [
    "# create list for diseased reddit images\n",
    "diseasedreddit_arrays = []\n",
    "# define filepath for diseased reddit images\n",
    "diseasedreddit_path = 'Data/reddit/diseased/'\n",
    "\n",
    "# convert each image to normalized array and store\n",
    "count = 0\n",
    "for file in os.listdir(diseasedreddit_path):\n",
    "    if count < 141:\n",
    "        try:\n",
    "            # target_size automatically resizes each img on import\n",
    "            diseased = load_img(diseasedreddit_path + file, target_size=(256, 256))\n",
    "            diseased_arr = img_to_array(diseased) / 255\n",
    "            diseasedreddit_arrays.append(diseased_arr)\n",
    "        except:\n",
    "            print(f'Error for file: {file}')\n",
    "        count +=1\n",
    "\n",
    "print(f'{len(diseasedreddit_arrays)} pictures converted.')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174 pictures converted.\n"
     ]
    }
   ],
   "source": [
    "# create list for wilted reddit images\n",
    "wiltedreddit_arrays = []\n",
    "# define filepath for wilted reddit images\n",
    "wiltedreddit_path = 'Data/reddit/wilted/'\n",
    "\n",
    "# convert each image to normalized array and store\n",
    "count = 0\n",
    "for file in os.listdir(wiltedreddit_path):\n",
    "    if count < 174:\n",
    "        try:\n",
    "            # target_size automatically resizes each img on import\n",
    "            diseased = load_img(wiltedreddit_path + file, target_size=(256, 256))\n",
    "            diseased_arr = img_to_array(diseased) / 255\n",
    "            wiltedreddit_arrays.append(diseased_arr)\n",
    "        except:\n",
    "            print(f'Error for file: {file}')\n",
    "        count +=1\n",
    "\n",
    "print(f'{len(wiltedreddit_arrays)} pictures converted.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2216, 256, 256, 3)\n",
      "y shape: (2216,)\n"
     ]
    }
   ],
   "source": [
    "# X should contain all converted images\n",
    "X = healthycrop_arrays + healthyhouse_arrays + diseasedcrop_arrays + wiltedhouse_arrays + diseasedreddit_arrays + wiltedreddit_arrays\n",
    "# convert to array and check shape\n",
    "X_arr = np.array(X)\n",
    "print(f'X shape: {X_arr.shape}')\n",
    "\n",
    "# 0 for healthy, 1 for not healthy\n",
    "y = [0] * 951 + [1] * 951 + [1] * 314\n",
    "# convert to array and check shape\n",
    "y = np.array(y)\n",
    "print(f'y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1662 samples, validate on 554 samples\n",
      "Epoch 1/10\n",
      "1662/1662 [==============================] - 117s 70ms/sample - loss: 0.6814 - accuracy: 0.5620 - val_loss: 0.6612 - val_accuracy: 0.5704\n",
      "Epoch 2/10\n",
      "1662/1662 [==============================] - 116s 70ms/sample - loss: 0.6464 - accuracy: 0.6113 - val_loss: 0.6301 - val_accuracy: 0.7509\n",
      "Epoch 3/10\n",
      "1662/1662 [==============================] - 115s 69ms/sample - loss: 0.5991 - accuracy: 0.6955 - val_loss: 0.6750 - val_accuracy: 0.6227\n",
      "Epoch 4/10\n",
      "1662/1662 [==============================] - 115s 69ms/sample - loss: 0.5557 - accuracy: 0.7377 - val_loss: 0.5154 - val_accuracy: 0.7888\n",
      "Epoch 5/10\n",
      "1662/1662 [==============================] - 115s 69ms/sample - loss: 0.5095 - accuracy: 0.7503 - val_loss: 0.5112 - val_accuracy: 0.7924\n",
      "Epoch 6/10\n",
      "1662/1662 [==============================] - 114s 69ms/sample - loss: 0.4941 - accuracy: 0.7647 - val_loss: 0.5341 - val_accuracy: 0.7762\n",
      "Epoch 7/10\n",
      "1662/1662 [==============================] - 114s 69ms/sample - loss: 0.4615 - accuracy: 0.7762 - val_loss: 0.4980 - val_accuracy: 0.7834\n",
      "Epoch 8/10\n",
      "1662/1662 [==============================] - 115s 69ms/sample - loss: 0.4528 - accuracy: 0.7870 - val_loss: 0.4905 - val_accuracy: 0.8032\n",
      "Epoch 9/10\n",
      "1662/1662 [==============================] - 116s 70ms/sample - loss: 0.3951 - accuracy: 0.8243 - val_loss: 0.5801 - val_accuracy: 0.7310\n",
      "Epoch 10/10\n",
      "1662/1662 [==============================] - 119s 72ms/sample - loss: 0.4538 - accuracy: 0.7894 - val_loss: 0.4973 - val_accuracy: 0.7906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe8affbebd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build CNN model 1\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu', input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D(pool_size= (2,2)))\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "# flatten and make dense\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16,activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          batch_size=64,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score was in epoch 10 with a train accuracy of 79% and test accuracy of 79%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1662 samples, validate on 554 samples\n",
      "Epoch 1/10\n",
      "1662/1662 [==============================] - 119s 72ms/sample - loss: 0.7011 - accuracy: 0.5572 - val_loss: 0.6812 - val_accuracy: 0.5704\n",
      "Epoch 2/10\n",
      "1662/1662 [==============================] - 117s 70ms/sample - loss: 0.6500 - accuracy: 0.6402 - val_loss: 0.5972 - val_accuracy: 0.7437\n",
      "Epoch 3/10\n",
      "1662/1662 [==============================] - 118s 71ms/sample - loss: 0.5735 - accuracy: 0.7076 - val_loss: 0.5326 - val_accuracy: 0.7617\n",
      "Epoch 4/10\n",
      "1662/1662 [==============================] - 119s 71ms/sample - loss: 0.5820 - accuracy: 0.6877 - val_loss: 0.5530 - val_accuracy: 0.7671\n",
      "Epoch 5/10\n",
      "1662/1662 [==============================] - 118s 71ms/sample - loss: 0.5274 - accuracy: 0.7491 - val_loss: 0.5112 - val_accuracy: 0.8051\n",
      "Epoch 6/10\n",
      "1662/1662 [==============================] - 117s 70ms/sample - loss: 0.4971 - accuracy: 0.7677 - val_loss: 0.4901 - val_accuracy: 0.7942\n",
      "Epoch 7/10\n",
      "1662/1662 [==============================] - 117s 71ms/sample - loss: 0.4912 - accuracy: 0.7756 - val_loss: 0.4990 - val_accuracy: 0.8032\n",
      "Epoch 8/10\n",
      "1662/1662 [==============================] - 117s 71ms/sample - loss: 0.4644 - accuracy: 0.7972 - val_loss: 0.4743 - val_accuracy: 0.8014\n",
      "Epoch 9/10\n",
      "1662/1662 [==============================] - 118s 71ms/sample - loss: 0.4426 - accuracy: 0.8057 - val_loss: 0.4700 - val_accuracy: 0.7888\n",
      "Epoch 10/10\n",
      "1662/1662 [==============================] - 118s 71ms/sample - loss: 0.4335 - accuracy: 0.8153 - val_loss: 0.4715 - val_accuracy: 0.8032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe908b60310>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build CNN model 2\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu', input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D(pool_size= (2,2)))\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "# flatten and make dense\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          batch_size=64,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score was in epoch 10 with a train accuracy of 82% and test accuracy of 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1662 samples, validate on 554 samples\n",
      "Epoch 1/10\n",
      "1662/1662 [==============================] - 117s 70ms/sample - loss: 0.6332 - accuracy: 0.6420 - val_loss: 0.5912 - val_accuracy: 0.7238\n",
      "Epoch 2/10\n",
      "1662/1662 [==============================] - 114s 69ms/sample - loss: 0.5447 - accuracy: 0.7353 - val_loss: 0.5575 - val_accuracy: 0.7419\n",
      "Epoch 3/10\n",
      "1662/1662 [==============================] - 114s 69ms/sample - loss: 0.5068 - accuracy: 0.7575 - val_loss: 0.5161 - val_accuracy: 0.7744\n",
      "Epoch 4/10\n",
      "1662/1662 [==============================] - 114s 68ms/sample - loss: 0.4829 - accuracy: 0.7804 - val_loss: 0.4914 - val_accuracy: 0.7744\n",
      "Epoch 5/10\n",
      "1662/1662 [==============================] - 114s 69ms/sample - loss: 0.4364 - accuracy: 0.8093 - val_loss: 0.4873 - val_accuracy: 0.7780\n",
      "Epoch 6/10\n",
      "1662/1662 [==============================] - 114s 68ms/sample - loss: 0.4064 - accuracy: 0.8249 - val_loss: 0.5716 - val_accuracy: 0.7365\n",
      "Epoch 7/10\n",
      "1662/1662 [==============================] - 115s 69ms/sample - loss: 0.4680 - accuracy: 0.7792 - val_loss: 0.4667 - val_accuracy: 0.7960\n",
      "Epoch 8/10\n",
      "1662/1662 [==============================] - 114s 69ms/sample - loss: 0.3760 - accuracy: 0.8520 - val_loss: 0.4571 - val_accuracy: 0.7960\n",
      "Epoch 9/10\n",
      "1662/1662 [==============================] - 115s 69ms/sample - loss: 0.3559 - accuracy: 0.8406 - val_loss: 0.4813 - val_accuracy: 0.7834\n",
      "Epoch 10/10\n",
      "1662/1662 [==============================] - 113s 68ms/sample - loss: 0.3385 - accuracy: 0.8532 - val_loss: 0.5166 - val_accuracy: 0.7708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe919c14550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build CNN model 3\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu', input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D(pool_size= (2,2)))\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(16,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "# flatten and make dense\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          batch_size=64,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score was in epoch 8 with a train accuracy of 85% and test accuracy of 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1662 samples, validate on 554 samples\n",
      "Epoch 1/10\n",
      "1662/1662 [==============================] - 252s 151ms/sample - loss: 0.7491 - accuracy: 0.5535 - val_loss: 0.6500 - val_accuracy: 0.5957\n",
      "Epoch 2/10\n",
      "1662/1662 [==============================] - 234s 141ms/sample - loss: 0.6040 - accuracy: 0.6649 - val_loss: 0.5693 - val_accuracy: 0.7094\n",
      "Epoch 3/10\n",
      "1662/1662 [==============================] - 236s 142ms/sample - loss: 0.5292 - accuracy: 0.7335 - val_loss: 0.5398 - val_accuracy: 0.7238\n",
      "Epoch 4/10\n",
      "1662/1662 [==============================] - 246s 148ms/sample - loss: 0.5094 - accuracy: 0.7557 - val_loss: 0.5727 - val_accuracy: 0.6968\n",
      "Epoch 5/10\n",
      "1662/1662 [==============================] - 233s 140ms/sample - loss: 0.4438 - accuracy: 0.7816 - val_loss: 0.5745 - val_accuracy: 0.7347\n",
      "Epoch 6/10\n",
      "1662/1662 [==============================] - 235s 141ms/sample - loss: 0.3630 - accuracy: 0.8442 - val_loss: 0.4873 - val_accuracy: 0.7960\n",
      "Epoch 7/10\n",
      "1662/1662 [==============================] - 243s 146ms/sample - loss: 0.3345 - accuracy: 0.8592 - val_loss: 0.6134 - val_accuracy: 0.7347\n",
      "Epoch 8/10\n",
      "1662/1662 [==============================] - 250s 150ms/sample - loss: 0.2556 - accuracy: 0.8971 - val_loss: 0.5589 - val_accuracy: 0.7798\n",
      "Epoch 9/10\n",
      "1662/1662 [==============================] - 243s 146ms/sample - loss: 0.2188 - accuracy: 0.9073 - val_loss: 0.7708 - val_accuracy: 0.7310\n",
      "Epoch 10/10\n",
      "1662/1662 [==============================] - 239s 144ms/sample - loss: 0.1337 - accuracy: 0.9537 - val_loss: 0.7591 - val_accuracy: 0.7690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fef1c55b050>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build CNN model 4\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D(pool_size= (2,2)))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size= (2,2), strides=(2,2)))\n",
    "\n",
    "# flatten and make dense\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          batch_size=64,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score was in epoch 6 with a train accuracy of 84% and test accuracy of 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nnet)",
   "language": "python",
   "name": "nnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
